{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:35.848805Z","iopub.execute_input":"2023-06-02T17:24:35.849345Z","iopub.status.idle":"2023-06-02T17:24:35.855980Z","shell.execute_reply.started":"2023-06-02T17:24:35.849296Z","shell.execute_reply":"2023-06-02T17:24:35.855051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\n\ncurrent_dir = '/kaggle/input/2023sumdpl302m'\n# Đường dẫn tới thư mục chứa dữ liệu ảnh\nimage_folder = os.path.join(current_dir, \"devset_images/devset_images\")\n\n# Đường dẫn tới file CSV chứa nhãn của các ảnh\ncsv_file = os.path.join(current_dir, \"devset_images_gt.csv\")\n\n# Đọc dữ liệu từ file CSV\ndata = pd.read_csv(csv_file)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:35.858208Z","iopub.execute_input":"2023-06-02T17:24:35.859273Z","iopub.status.idle":"2023-06-02T17:24:35.875076Z","shell.execute_reply.started":"2023-06-02T17:24:35.859238Z","shell.execute_reply":"2023-06-02T17:24:35.874159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:35.876972Z","iopub.execute_input":"2023-06-02T17:24:35.877453Z","iopub.status.idle":"2023-06-02T17:24:35.891992Z","shell.execute_reply.started":"2023-06-02T17:24:35.877405Z","shell.execute_reply":"2023-06-02T17:24:35.890871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass FloodDataset(Dataset):\n    def __init__(self, row, exact_dir, fold=\"train\", transform=None):\n        self.row = row\n        self.transform = transform\n        self.exact_dir = exact_dir\n        self.fold = fold\n\n    def __len__(self):\n        return len(self.row.id)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(current_dir, self.exact_dir,f'{list(self.row.id)[idx]}.jpg')\n        if os.path.exists(image_path):\n            image = Image.open(image_path)\n        else:\n            image_path = os.path.join(current_dir, self.exact_dir,f'{list(self.row.id)[idx]}.png')\n            if os.path.exists(image_path):\n                image = Image.open(image_path)\n            else:\n                image_path = os.path.join(current_dir, self.exact_dir,f'{list(self.row.id)[idx]}.gif')\n                image = Image.open(image_path)\n        if self.transform:\n            image = self.transform[self.fold](image)\n            \n        if (self.fold == \"test\"):\n            return image, list(self.row.id)[idx]\n        label = list(self.row.label)[idx]\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:35.893976Z","iopub.execute_input":"2023-06-02T17:24:35.894437Z","iopub.status.idle":"2023-06-02T17:24:35.907995Z","shell.execute_reply.started":"2023-06-02T17:24:35.894377Z","shell.execute_reply":"2023-06-02T17:24:35.906887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = FloodDataset(data,exact_dir = \"devset_images/devset_images\", transform= None)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:35.911687Z","iopub.execute_input":"2023-06-02T17:24:35.912195Z","iopub.status.idle":"2023-06-02T17:24:35.919506Z","shell.execute_reply.started":"2023-06-02T17:24:35.912154Z","shell.execute_reply":"2023-06-02T17:24:35.918385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize variables to accumulate sums\n# sum_channels = torch.zeros(3)\n# sum_channels_squared = torch.zeros(3)\n# transform = transforms.ToTensor()\n\n# # Iterate over the dataset to compute sums\n# for image, _ in dataset:\n#     image = transform(image)\n#     sum_channels += torch.sum(image, dim=(1, 2))  # Sum the values in each channel\n#     sum_channels_squared += torch.sum(image ** 2, dim=(1, 2))  # Sum the squared values in each channel\n\n# # Calculate the mean and standard deviation\n# num_images = len(dataset)\n# mean = sum_channels / (num_images * image.size(1) * image.size(2))\n# std = torch.sqrt((sum_channels_squared / (num_images * image.size(1) * image.size(2))) - mean ** 2)\n\n# print(\"Mean:\", mean)\n# print(\"Std:\", std)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:35.921393Z","iopub.execute_input":"2023-06-02T17:24:35.921859Z","iopub.status.idle":"2023-06-02T17:24:35.930798Z","shell.execute_reply.started":"2023-06-02T17:24:35.921823Z","shell.execute_reply":"2023-06-02T17:24:35.929590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'label {dataset[13][1]}')\ndataset[13][0]","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:35.932445Z","iopub.execute_input":"2023-06-02T17:24:35.933309Z","iopub.status.idle":"2023-06-02T17:24:36.024691Z","shell.execute_reply.started":"2023-06-02T17:24:35.933273Z","shell.execute_reply":"2023-06-02T17:24:36.023847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_labels = data[\"label\"].unique()\nprint(unique_labels)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:36.026109Z","iopub.execute_input":"2023-06-02T17:24:36.027217Z","iopub.status.idle":"2023-06-02T17:24:36.034603Z","shell.execute_reply.started":"2023-06-02T17:24:36.027186Z","shell.execute_reply":"2023-06-02T17:24:36.033638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport transformers\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom torchvision.models.resnet import BasicBlock, ResNet18_Weights\n\ndef get_model_resnet(num_classes):\n    model_ft = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    num_fts = model_ft.fc.in_features\n    # Here the size of each output sample is set to 2.\n    # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n    # Change fc network to other custome fc network\n    model_ft.fc = nn.Linear(num_fts, num_classes)  \n    \n    return model_ft\n\ndef get_model_resnet_with_freeze_weights(num_classes):\n    model_ft = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    num_fts = model_ft.fc.in_features\n    # Here the size of each output sample is set to 2.\n    # Change fc network to other custome fc network\n    # model_ft.fc = nn.Linear(num_fts,num_class)\n    model_ft.fc = nn.Identity()\n\n    # freeze the weights\n    for param in model_ft.parameters():\n        param.requires_grad = False\n        \n    # Add a new linear layer\n    model_ft.fc = nn.Sequential(\n        nn.Linear(num_fts, 64),\n        nn.ReLU(inplace = True),\n        nn.Linear(64, num_classes)\n    )\n       \n    # only train the fc_new layer\n    for param in model_ft.fc.parameters():\n        param.requires_grad = True\n        \n    return model_ft\n\ndef get_model_vit(num_classes):\n    # processor for the input image\n    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n    # model\n    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n    \n    # get in feature space for classifications\n    num_fts = model.classifier.in_features\n    \n    # change the classifier layer\n    model.classifier = nn.Sequential(\n        # nn.Linear(num_fts, 64),\n        # nn.ReLU(inplace=True),\n        # nn.Linear(64, num_classes)\n        nn.Linear(num_fts, num_classes)\n    )\n    return model\n\ndef get_model_vit_with_freeze_weights(num_classes):\n    # processor for the input image\n    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\n    # model\n    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n    \n    # get in feature space for classifications\n    num_fts = model.classifier.in_features\n    \n    for param in model.parameters():\n        param.requires_grad = False\n        \n    # change the classifier layer\n    model.classifier = nn.Sequential(\n        # nn.Linear(num_fts, 64),\n        # nn.ReLU(inplace=True),\n        # nn.Linear(64, num_classes)\n        nn.Linear(num_fts, num_classes)\n    )\n    \n    for param in model.classifier.parameters():\n        param.requires_grad = True\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:36.046506Z","iopub.execute_input":"2023-06-02T17:24:36.047261Z","iopub.status.idle":"2023-06-02T17:24:36.061812Z","shell.execute_reply.started":"2023-06-02T17:24:36.047221Z","shell.execute_reply":"2023-06-02T17:24:36.060757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# loss function\ncriterion = torch.nn.CrossEntropyLoss() \nmodel = get_model_vit(num_classes=2)\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\n# model to gpu\nmodel.to(device)\nlr= 1e-5\n\noptimizer = torch.optim.Adam(model.parameters(), lr)\n# auto decrease learning rate\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=4, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:36.064374Z","iopub.execute_input":"2023-06-02T17:24:36.064783Z","iopub.status.idle":"2023-06-02T17:24:37.201470Z","shell.execute_reply.started":"2023-06-02T17:24:36.064747Z","shell.execute_reply":"2023-06-02T17:24:37.200534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = {\"train\": transforms.Compose([\n    transforms.Resize((224, 224)),\n#     transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n    transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n]), \n   \"test\": transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n#     transforms.Normalize(mean = (0.4343, 0.4377, 0.4110), std = (0.2764, 0.2774, 0.2992)) \n    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n   ])\n}\n\ndataset = FloodDataset(data, exact_dir = \"devset_images/devset_images\", transform=transform, fold = \"train\")\n\ntrain_size = int(0.8 * len(dataset))\ntest_size = int(0.2 * len(dataset))\ntrain_set, val_set = torch.utils.data.random_split(dataset, [train_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:37.203313Z","iopub.execute_input":"2023-06-02T17:24:37.203662Z","iopub.status.idle":"2023-06-02T17:24:37.212718Z","shell.execute_reply.started":"2023-06-02T17:24:37.203630Z","shell.execute_reply":"2023-06-02T17:24:37.211549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nbatch_size = 32\nnum_epochs = 4\n# trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\ntrainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\ntrain_size = len(dataset)\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    running_corrects=0\n    for i, (inputs, labels) in tqdm(enumerate(trainloader)):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs.logits, labels)\n        \n        _, preds = torch.max(outputs.logits, 1)\n        running_corrects += torch.sum(preds == labels.data)\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    running_loss = running_loss / (train_size // batch_size)\n    running_corrects = running_corrects / train_size\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {running_loss/100:.4f}, Acc: {running_corrects:.4f}\")\n    \n    # Evaluation\n#     model.eval()\n#     correct = 0\n#     total = 0\n\n#     testloader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False, num_workers=2)\n#     with torch.no_grad():\n#         for images, labels in testloader:\n#             images = images.to(device)\n#             labels = labels.to(device)\n#             outputs = model(images)\n#             _, predicted = torch.max(outputs.logits, 1)\n#             total += labels.size(0)\n#             correct += (predicted == labels).sum().item()\n\n#     accuracy = 100 * correct / total\n#     print(f\"Accuracy on test set: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:24:37.214562Z","iopub.execute_input":"2023-06-02T17:24:37.215007Z","iopub.status.idle":"2023-06-02T17:25:03.152238Z","shell.execute_reply.started":"2023-06-02T17:24:37.214975Z","shell.execute_reply":"2023-06-02T17:25:03.150128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/2023sumdpl302m/test.csv\")\ntest_data['id'] = test_data.image_id","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:25:03.153650Z","iopub.status.idle":"2023-06-02T17:25:03.155113Z","shell.execute_reply.started":"2023-06-02T17:25:03.154841Z","shell.execute_reply":"2023-06-02T17:25:03.154868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer = {\"id\": [], \"label\": []}","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:25:03.156741Z","iopub.status.idle":"2023-06-02T17:25:03.157607Z","shell.execute_reply.started":"2023-06-02T17:25:03.157326Z","shell.execute_reply":"2023-06-02T17:25:03.157351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset = FloodDataset(test_data, exact_dir = 'testset_images/testset_images',fold=\"test\", transform=transform)\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\nwith torch.no_grad():\n    for images, id in testloader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.logits, 1)\n        id = id.tolist()\n        predicted = predicted.tolist()\n        answer['id'].extend(id)\n        answer['label'].extend(predicted)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:25:03.159117Z","iopub.status.idle":"2023-06-02T17:25:03.159894Z","shell.execute_reply.started":"2023-06-02T17:25:03.159640Z","shell.execute_reply":"2023-06-02T17:25:03.159663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_df = pd.DataFrame.from_dict(answer)","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:25:03.161253Z","iopub.status.idle":"2023-06-02T17:25:03.162053Z","shell.execute_reply.started":"2023-06-02T17:25:03.161788Z","shell.execute_reply":"2023-06-02T17:25:03.161811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_df.drop_duplicates(subset='id').to_csv(\"submission.csv\", index=Falsetransform","metadata":{"execution":{"iopub.status.busy":"2023-06-02T17:25:03.163466Z","iopub.status.idle":"2023-06-02T17:25:03.164233Z","shell.execute_reply.started":"2023-06-02T17:25:03.163995Z","shell.execute_reply":"2023-06-02T17:25:03.164017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}